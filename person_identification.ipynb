{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49158e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59ef94",
   "metadata": {},
   "source": [
    "Resizing images:\n",
    "\n",
    "Images are resized to 128x128 to increase speed but were not resized to much smaller size to retain information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03689b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scale_h,scale_w=32,32\n",
    "\n",
    "for j in range(1,6):\n",
    "    folder=f\"dataset\\\\classes\\\\{j}\"\n",
    "    for i in range(1,21):\n",
    "        path = f\"{folder}\\\\{i}.png\"\n",
    "        image = cv2.imread(path)\n",
    "\n",
    "        if image is None:\n",
    "            print(f\"Image at path {path} could not be loaded.\")\n",
    "            continue\n",
    "\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        height, width = image_rgb.shape[1],image_rgb.shape[2]\n",
    "        if height > scale_h or width > scale_w: \n",
    "            image_rgb = cv2.resize(image_rgb, (scale_h,scale_w))\n",
    "\n",
    "        pil_image = Image.fromarray(image_rgb)\n",
    "        pil_image.save(os.path.join(folder, f\"{i}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acdaa37",
   "metadata": {},
   "source": [
    "Augmentation of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random rotation of images \n",
    "\n",
    "def random_rotate(image, angle_range=(-15, 15)):\n",
    "    angle = random.uniform(*angle_range)\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "\n",
    "    # Rotation matrix\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    \n",
    "    rotated_o = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "    return rotated_o\n",
    "\n",
    "output_dir = \"dataset\\\\rotated images\"    \n",
    "        \n",
    "for j in range(1,6):\n",
    "    folder_o=f\"dataset\\\\classes\\\\{j}\"\n",
    "    \n",
    "    save_dir_o=os.path.join(output_dir,str(j))\n",
    "    os.makedirs(save_dir_o, exist_ok=True)\n",
    "    \n",
    "    for i in range(1,21):\n",
    "        path_o = f\"{folder_o}\\\\{i}.png\"\n",
    "        \n",
    "        #reading image and corresponding ghibli\n",
    "        image = cv2.imread(path_o,cv2.IMREAD_COLOR_RGB)\n",
    "        \n",
    "        rotated_img = random_rotate(image)\n",
    "\n",
    "        pil_image = Image.fromarray(rotated_img)\n",
    "        pil_image.save(os.path.join(save_dir_o, f\"{i}.png\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd053a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly scaling images (cropping , down scaling)\n",
    "\n",
    "def random_scale(image, scale_range=(0.9, 1.1)):\n",
    "    h, w = image.shape[:2]\n",
    "    scale = random.uniform(*scale_range)\n",
    "\n",
    "    # Compute new size\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "\n",
    "    # Resize image\n",
    "    scaled = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Crop or pad to original size\n",
    "    top = max((new_h - h) // 2, 0)\n",
    "    bottom = max(h - new_h, 0) // 2\n",
    "    left = max((new_w - w) // 2, 0)\n",
    "    right = max(w - new_w, 0) // 2\n",
    "\n",
    "    # Crop if scaled up\n",
    "    if scale > 1.0:\n",
    "        scaled = scaled[top:top + h, left:left + w]\n",
    "        \n",
    "    # Pad if scaled down\n",
    "    else:\n",
    "        scaled = cv2.copyMakeBorder(scaled, bottom, h - new_h - bottom, right, w - new_w - right, borderType=cv2.BORDER_REFLECT)\n",
    "        \n",
    "    return scaled\n",
    "\n",
    "\n",
    "output_dir = \"dataset\\\\scaled images\"    \n",
    "\n",
    "for j in range(1,6):\n",
    "    folder_o=f\"dataset\\\\classes\\\\{j}\"\n",
    "    \n",
    "    save_dir_o=os.path.join(output_dir,str(j))\n",
    "    os.makedirs(save_dir_o, exist_ok=True)\n",
    "    \n",
    "    for i in range(1,21):\n",
    "        path_o = f\"{folder_o}\\\\{i}.png\"\n",
    "        \n",
    "        #reading image and corresponding ghibli\n",
    "        image = cv2.imread(path_o,cv2.IMREAD_COLOR_RGB)\n",
    "        \n",
    "        \n",
    "        scaled_img=random_scale(image)\n",
    "\n",
    "        pil_image = Image.fromarray(scaled_img)\n",
    "        pil_image.save(os.path.join(save_dir_o, f\"{i}.png\"))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_brightness_contrast(image, brightness_range=(-30, 30), contrast_range=(0.8, 1.2)):\n",
    "    # Random brightness shift\n",
    "    brightness = random.randint(*brightness_range)\n",
    "\n",
    "    # Random contrast factor\n",
    "    contrast = random.uniform(*contrast_range)\n",
    "\n",
    "    # Convert image to float32 for accurate math\n",
    "    img = image.astype(np.float32)\n",
    "    \n",
    "    # Apply contrast and brightness: new = image * contrast + brightness\n",
    "    img = img * contrast + brightness\n",
    "    \n",
    "    # Clip values to [0, 255] and convert back to uint8\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "output_dir = \"dataset\\\\contrast images\"    \n",
    "\n",
    "for j in range(1,6):\n",
    "    folder_o=f\"dataset\\\\classes\\\\{j}\"\n",
    "    \n",
    "    save_dir_o=os.path.join(output_dir,str(j))\n",
    "    os.makedirs(save_dir_o, exist_ok=True)\n",
    "    \n",
    "    for i in range(1,21):\n",
    "        path_o = f\"{folder_o}\\\\{i}.png\"\n",
    "        \n",
    "        #reading image and corresponding ghibli\n",
    "        image = cv2.imread(path_o,cv2.IMREAD_COLOR_RGB)\n",
    "        \n",
    "        \n",
    "        img=random_brightness_contrast(image)\n",
    "\n",
    "        pil_image = Image.fromarray(img)\n",
    "        pil_image.save(os.path.join(save_dir_o, f\"{i}.png\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99aec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN with 2 hidden layers\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(a):\n",
    "    return (a > 0).astype(float)\n",
    "\n",
    "def initialize_parameters(input_size, hidden_sizes, output_size):\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(input_size, hidden_sizes[0]) * 0.01\n",
    "    b1 = np.zeros((1, hidden_sizes[0]))\n",
    "    W2 = np.random.randn(hidden_sizes[0], hidden_sizes[1]) * 0.01\n",
    "    b2 = np.zeros((1, hidden_sizes[1]))\n",
    "    W3 = np.random.randn(hidden_sizes[1], output_size) * 0.01\n",
    "    b3 = np.zeros((1, output_size))\n",
    "\n",
    "    return {\n",
    "        \"W1\": W1, \"b1\": b1,\n",
    "        \"W2\": W2, \"b2\": b2,\n",
    "        \"W3\": W3, \"b3\": b3\n",
    "    }\n",
    "\n",
    "def forward_pass(X, params, apply_dropout, is_training, keep_prob=0.8):\n",
    "    Z1 = np.dot(X, params[\"W1\"]) + params[\"b1\"]\n",
    "    A1 = relu(Z1)\n",
    "    if apply_dropout and is_training:\n",
    "        dropout1 = (np.random.rand(*A1.shape) < keep_prob).astype(float)\n",
    "        A1 *= dropout1\n",
    "    else:\n",
    "        A1 *= keep_prob\n",
    "\n",
    "    Z2 = np.dot(A1, params[\"W2\"]) + params[\"b2\"]\n",
    "    A2 = relu(Z2)\n",
    "    if apply_dropout and is_training:\n",
    "        dropout2 = (np.random.rand(*A2.shape) < keep_prob).astype(float)\n",
    "        A2 *= dropout2\n",
    "    else:\n",
    "        A2 *= keep_prob\n",
    "\n",
    "    Z3 = np.dot(A2, params[\"W3\"]) + params[\"b3\"]\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    cache = {\n",
    "        \"A1\": A1, \"Z1\": Z1, \"dropout1\": dropout1 if apply_dropout else None,\n",
    "        \"A2\": A2, \"Z2\": Z2, \"dropout2\": dropout2 if apply_dropout else None,\n",
    "        \"A3\": A3, \"Z3\": Z3\n",
    "    }\n",
    "    return A3, cache\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    eps = 1e-8\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    return np.mean(y_pred_labels == y_true_labels) * 100\n",
    "\n",
    "def backward_pass(X, y, cache, params, learning_rate, keep_prob=1.0):\n",
    "    m = X.shape[0]\n",
    "    A1, A2, A3 = cache[\"A1\"], cache[\"A2\"], cache[\"A3\"]\n",
    "\n",
    "    dZ3 = A3 - y\n",
    "    dW3 = np.dot(A2.T, dZ3) / m\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA2 = np.dot(dZ3, params[\"W3\"].T)\n",
    "    dA2 *= relu_derivative(A2)\n",
    "    if cache[\"dropout2\"] is not None:\n",
    "        dA2 *= cache[\"dropout2\"]\n",
    "\n",
    "    dW2 = np.dot(A1.T, dA2) / m\n",
    "    db2 = np.sum(dA2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA1 = np.dot(dA2, params[\"W2\"].T)\n",
    "    dA1 *= relu_derivative(A1)\n",
    "    if cache[\"dropout1\"] is not None:\n",
    "        dA1 *= cache[\"dropout1\"]\n",
    "\n",
    "    dW1 = np.dot(X.T, dA1) / m\n",
    "    db1 = np.sum(dA1, axis=0, keepdims=True) / m\n",
    "\n",
    "    params[\"W3\"] -= learning_rate * dW3\n",
    "    params[\"b3\"] -= learning_rate * db3\n",
    "    params[\"W2\"] -= learning_rate * dW2\n",
    "    params[\"b2\"] -= learning_rate * db2\n",
    "    params[\"W1\"] -= learning_rate * dW1\n",
    "    params[\"b1\"] -= learning_rate * db1\n",
    "\n",
    "\n",
    "def train(X, y, hidden_sizes=[128, 64], output_size=5, epochs=100, learning_rate=0.1, keep_prob=0.8, patience=5):\n",
    "    input_size = X.shape[1]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "    params = initialize_parameters(input_size, hidden_sizes, output_size)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    wait = 0\n",
    "    history = {\"loss\": [], \"acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        y_pred, cache = forward_pass(X_train, params, apply_dropout=True, is_training=True, keep_prob=keep_prob)\n",
    "        loss = compute_loss(y_train, y_pred)\n",
    "        acc = compute_accuracy(y_train, y_pred)\n",
    "        backward_pass(X_train, y_train, cache, params, learning_rate, keep_prob)\n",
    "\n",
    "        val_pred, _ = forward_pass(X_val, params, apply_dropout=False, is_training=False)\n",
    "        val_loss = compute_loss(y_val, val_pred)\n",
    "        val_acc = compute_accuracy(y_val, val_pred)\n",
    "\n",
    "        history[\"loss\"].append(loss)\n",
    "        history[\"acc\"].append(acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f} , val Loss: {val_loss:.4f} , acc: {acc:.4f} , val acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return params, history\n",
    "\n",
    "def test(X, params):\n",
    "    y_pred, _ = forward_pass(X, params, apply_dropout=False, is_training=False)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da33c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(training_loss,v_test_loss,training_acc,v_test_acc):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_loss, label='Train Loss')\n",
    "    plt.plot(v_test_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    #accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(training_acc, label='Train accuracy')\n",
    "    plt.plot(v_test_acc, label='Validation accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('accuracy Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_simple_features(image):\n",
    "    image=cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    image=image.flatten()\n",
    "    image=image/255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d991c1b",
   "metadata": {},
   "source": [
    "DATASET IMAGES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8be384",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[\"yasir\",\"cheema\",\"arslan\",\"ahmed\",\"ebad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a22257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample of training images\n",
    "\n",
    "def plot_images(training_images,labels, img_index, fig_len):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    j = img_index\n",
    "    \n",
    "    for i in range(fig_len):\n",
    "        ax = fig.add_subplot(2, fig_len, i + 1)\n",
    "        ax.imshow(training_images[j])\n",
    "        ax.axis('off')\n",
    "        ax.set_title(labels[j])\n",
    "        j += 1\n",
    "images=[]\n",
    "\n",
    "for j in range(1,6):\n",
    "    folder_o=f\"dataset\\\\classes\\\\{j}\"\n",
    "    \n",
    "    for i in range(1,2):\n",
    "        path_o = f\"{folder_o}\\\\{i}.png\"\n",
    "        \n",
    "        #reading image\n",
    "        image_o = cv2.imread(path_o,cv2.IMREAD_COLOR_RGB)\n",
    "        images.append(image_o)\n",
    "        \n",
    "plot_images(training_images=images,labels=labels,img_index=0,fig_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_o,y_train,X_train_g=[],[],[]\n",
    "\n",
    "X_train_r,X_train_rg=[],[]\n",
    "\n",
    "X_train_s,X_train_sg=[],[]\n",
    "\n",
    "for j in range(1,6):\n",
    "    \n",
    "    folder_o=f\"dataset\\\\classes\\\\{j}\"\n",
    "    \n",
    "    folder_r=f\"dataset\\\\rotated images\\\\{j}\"\n",
    "    \n",
    "    folder_s=f\"dataset\\\\scaled images\\\\{j}\"\n",
    "    \n",
    "    \n",
    "    for i in range(1,21):\n",
    "        path_o = f\"{folder_o}\\\\{i}.png\"\n",
    "        \n",
    "        path_r = f\"{folder_r}\\\\{i}.png\"\n",
    "        \n",
    "        path_s = f\"{folder_s}\\\\{i}.png\"\n",
    "        \n",
    "        \n",
    "        image_o = cv2.imread(path_o,cv2.IMREAD_COLOR_RGB)\n",
    "        \n",
    "        image_r = cv2.imread(path_r,cv2.IMREAD_COLOR_RGB)\n",
    "        \n",
    "        image_s = cv2.imread(path_s,cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "        #extracting the  features of images and their corresponding ghiblis\n",
    "        \n",
    "        features_o=extract_simple_features(image_o)\n",
    "\n",
    "        features_r=extract_simple_features(image_r)\n",
    "\n",
    "        features_s=extract_simple_features(image_s)\n",
    "\n",
    "       \n",
    "        #storing particular features\n",
    "        X_train_o.append(features_o)\n",
    "\n",
    "        X_train_r.append(features_r)\n",
    "\n",
    "        X_train_s.append(features_s)\n",
    "\n",
    "        #one hot-encoded vector        \n",
    "        new_labels=[0]*len(labels)\n",
    "      \n",
    "        label_idx = j - 1  \n",
    "        new_labels[label_idx] = 1\n",
    "        \n",
    "        y_train.append(new_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f5e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to numpy arrays\n",
    "X_train=[X_train_o,X_train_r,X_train_s]\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i]=np.array(X_train[i])\n",
    "\n",
    "\n",
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating all images,labels into one array\n",
    "X_total = np.concatenate(X_train)\n",
    "y_total = np.concatenate([y_train] * len(X_train), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_total.shape)\n",
    "print(y_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_total,y_total,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=(X_train-np.mean(X_train))/np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753afd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, history = train(X_train, y_train, hidden_sizes=[512,128], epochs=1000, learning_rate=0.01, keep_prob=0.8, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96784f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history['loss'],history['val_loss'],history['acc'],history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights and biases in a .npz file\n",
    "\n",
    "np.savez(\"model_weights.npz\", **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1505c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights before testing\n",
    "\n",
    "loaded_weights=np.load(\"model_weights.npz\")\n",
    "params={key: loaded_weights[key] for key in loaded_weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=(X_test-np.mean(X_test))/np.std(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=test(X_test,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing test accuracy\n",
    "test_acc=compute_accuracy(y_test,y_pred)\n",
    "print(f\"test accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
